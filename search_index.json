[["index.html", "Introduction to Statistics and Computation with Data Preface", " Introduction to Statistics and Computation with Data Siva Athreya Bikram Halder Preface Authors Siva Athreya (athreya@isibang.ac.in) Bikram Halder (bmat2112@isibang.ac.in) Chapters R basics and Exploratory data analysis R basics Installation and setup Queries regarding operating R and RStudio Data Types and description Playing with Data in R Data frames in R The tidyverse-package Reading Data frames into R Generating Random Data sets in R Working With dplyr-package Factors and Levels Date and Time in R Data visualization ggplot2 basics Aesthetic mappings Facets (wrap and grid) Geometric objects Statistical transformations Position adjustments Coordinate systems The layered grammar of graphics Review of Basic Probability Recalling the distributions, Expectation, Variance DeMoivre Central Limit Theorem Tchebychev Inequality Empirical Distribution Function Sample Mean and Variance \\(\\chi^2, t\\) and \\(F\\) distribution Statistical Estimation Estimator MOM MLE Confidence Intervals Bivariate-Data Covariance, Correlation Method of Least squares (Linear regression) \\(t\\)-confidence Interval Hypothesis Testing Intro \\(z\\) and \\(t\\) test \\(\\chi^2\\) goodness of fit Non-parametric tests: Sign, Sign-ranked Test (Wilcox), Fisher Exact test for independence Resampling Methods: Jackknife and Bootstrap "],["1-r-basics-and-exploratory-data-analysis.html", "Chapter 1 R basics and Exploratory data analysis", " Chapter 1 R basics and Exploratory data analysis “Code is like humor. When you have to explain it, it’s bad.” — Cory House R is a great language heavily used by both academics and industrials for Machine Learning, Data Science and Deep Learning related stuffs. In this chapter, we will give you introduction to R, how to work with various R commands, plottig and visualizing data with R and how you can analyse and explore data using R. "],["1.1-r-lang.html", "1.1 Introduction", " 1.1 Introduction R is a free and open-source interpreted computer programming language and runs on Linux, Windows, and macOS. It is modeled after S and S-Plus. The S language was developed in the late 1980s at AT &amp; T labs. The R project was started by Robert Gentleman and Ross Ihaka of the Statistics Department of the University of Auckland in 1995. It is now a collaborative project with many contributors and is maintained by the R core-development team. 1.1.1 Installation and setup To download R, go to CRAN, the Comprehensive R Archive Network. CRAN is composed of a set of mirror servers distributed around the world and is used to distribute R and R packages. Don’t try and pick a mirror that’s close to you: instead use the cloud mirror, https://cloud.r-project.org, which automatically figures it out for you. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions, which require you to reinstall all your packages, but putting it off only makes it worse. 1.1.1.1 RStudio RStudio is an IDE (integrated development environment) exclusively for R programming. Download and install it from https://www.rstudio.com/download. RStudio is updated a couple of times a year. When a new version is available, RStudio will let you know. It’s a good idea to upgrade regularly so you can take advantage of the latest and greatest features. When you start RStudio, you will see two key regions in the interface: RStudio Screenshot 1.1.1.2 R Packages Sometimes you may need to install some additional packages. An R package is a collection of functions, datasets, documentation that extends the capabilities of base-R. Using packages you can enhance your data exploration capabilities in R. You can install an R package in this way. install.packages(&quot;UsingR&quot;) Once installed you can add them in the current workspace by using library() function library(&quot;UsingR&quot;) 1.1.1.3 R help R help is a great tool to make youself familiar with unknown R stuffs. All the R packages are well documented. You can get information about any function, dataset, package which are currently installed by just executing ?object_name It will open a documentation in the bottom right (where you saw the plot) section of RStudio. And to get help for any keyword execute ??keyword It will search throughout the R documentations of the currently installed packages and show all the results related to that keyword. 1.1.2 Queries regarding operating R and RStudio So far we have done some basic setup for R. Let’s do some proper coding so that we make ourself familiar with this language. 1.1.2.1 Basic Coding in R R can be used as calculator: 9 / 44 #&gt; [1] 0.2045455 0.6 * 0.4 + 0.3 * 0.6 #&gt; [1] 0.42 log(0.6 * 0.4 + 0.3 * 0.6) #&gt; [1] -0.8675006 1.1.2.2 The c() function Suppose we wish to enter scores in Computer Science class of 10 students. The scores are 40, 39, 15, 6, 18, 22, 30, 21, 15, 23. Scores &lt;- c(40, 39, 15, 6, 18, 22, 30, 21, 15, 23) c() is generic function in R which creates a vector or list of values with all of it’s arguments coerced to a common type. Note: We have assigned values to a variable Scores with the assignment operator &lt;-. = can also be used for assignment but throughout this book we will use R-community preferred one ,i.e., &lt;-. Sometimes we become lazy and use = instead of &lt;- but don’t worry, you can use a RStudio keyboard shoutcut: Alt + -(yes, the minus sign). The value of a variable don’t get automatically displayed unless we call the variable. Scores #&gt; [1] 40 39 15 6 18 22 30 21 15 23 1.1.2.3 Inbuilt functions Almost everything in R is done through functions. It has a lot of inbuilt functions. (40 + 39 + 15 + 6 + 18 + 22 + 30 + 21 + 15 + 23) / 10 #&gt; [1] 22.9 Instead of writing such manaul expression we can just use the mean() function. mean(Scores) #&gt; [1] 22.9 sd(Scores) #&gt; [1] 10.75433 No! sd() doesn’t compute the positive square root of variance. It calculates the positive square root of sample variance, which is kind of same as variance but division is done by 1 less than the total number of observations instead of total number of observations. That is, if we have observations \\(X_1, X_2, \\ldots, X_n\\), then sample variance, \\[ S^2 = \\frac{1}{n-1}\\sum_{j=1}^n (X_i - \\overline{X})^2 \\] R functions are called like this: function_name(arg_1 = val1, arg_2 = val2, ...) You can also call the functions in this way: function_name(val1, val2, ...) But latter one is risky! You have to know the order of the arguments given in the function definition. The former one is recommended in practice. It is better if you are using a function first time. No matter in what order the arguments are defined, the values can be passed more easily to the proper arguments. 1.1.2.4 Manipulating vectors Now we are going to show some useful vector manipulations in R. These will extremely helpful when you will manipulate datsets. As discussed earlier c() is used to create vector with elements coerced to a same type. Individual elements can be accessed by specifying the index inside [] after the variable. Suppose we want to change the entry of student 4 from 6 to 16 in Scores. Scores[4] &lt;- 16 Scores #&gt; [1] 40 39 15 16 18 22 30 21 15 23 We accessed the 4th entry by Scores[4] and using assignment operator we have changed the value. Note: Unlike other programming languages, indexing of elements in vectors, lists, data frames, tibbles and other multi-valued data types in R starts from 1 not 0 ! Selecting some entries of Scores Scores[c(1, 3, 5)] #&gt; [1] 40 15 18 Suppose we want to see which students’ Scores are equal to 30 marks which(Scores == 30) #&gt; [1] 7 or those with Scores lesser than or equal to 20. which(Scores &lt;= 20) #&gt; [1] 3 4 5 9 x &lt;- 1:100 x #&gt; [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #&gt; [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #&gt; [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 #&gt; [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 #&gt; [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 #&gt; [91] 91 92 93 94 95 96 97 98 99 100 x[x &lt; 10 | x &gt; 90] #&gt; [1] 1 2 3 4 5 6 7 8 9 91 92 93 94 95 96 97 98 99 100 Exercises Hafisa has a log book of time spentby her on Moodle. In the log book she keeps track of the 24-hour reading before each time she logs in. The last 10 readings for a particular student in a day are: \\[ 7, 8, 9, 11, 13, 15, 17, 18, 23 \\] Enter these numbers into R as a variable moodle_reading. Use the function diff() on the data. What does it give? Write down, x, the number of hours between each time Hafisa logs into moodle. Use the max() to find the maximum number of hours, the mean() function to find the average number of hours and the min() to get the minimum number of hours for Hafisa between two logins. Happy Gouda’s quiz scores in Semester II are given below \\[ 7, 6, 10, 8, 7, 9, 9, 6, 4, 10, 8, 6, 9, 10 \\] Enter this into R as a variable score_hg. Use the function max() to find the highest score, the function mean to find the average and the function min to find the minimum. When confronted by Looser Siva, HG realises that entry 4 was a mistake. It should have been 5. How can you fix this? Do so, and then find the new average. What does the below command provide in R? sum(score_hg &gt;= 9) What do you get? What percent of your scores are less than 17? How can you answer this with R? Naina’s cell phone bill varies from month to month. Suppose in her first year of Super DATA (hons.) program, under the Drop-atmost 10-calls monthly plan, the following monthly amounts were incurred: \\[ 460, 330, 390, 370, 460, 300, 480, 320, 490, 350, 300, 480 \\] Enter this data into a variable called naina_bill. Use the sum() function to find the amount spent by Naina that year on the cell phone. Using R find out what is the smallest amount she spent in a month and the largest amount she spent in a month? How many months was the amount greater than Rs 400? What percentage was this? Her monthly loan from NOmoney Bank was Rs 3000. Using R store her balance(after paying her phone bill) in a variable called free_money. Find the average amount available each month for her other expenses. "],["1.2-ggplot2-data-visualization.html", "1.2 ggplot2 data visualization", " 1.2 ggplot2 data visualization “The goal is to turn data into information and information into insight.” — Carly Fiorina This part will give you an understanding of data visualization using ggplot2. R does have many packages/methods to make graphs, infact base-R has plot, histogram etc. (as you have seen in the Data section) for plottig but ggplot2 is one of the most versatile one. It implements grammar of graphics, a powerful tool to describe and build the components of graphs concisely. If you are curious and want to get in-depth understanding of grammar of graphics in ggplot2 you can read The Layered Grammar of Graphics by Hadley Wickham. First install tidyverse which includes the ggplot2 package and then add to the current workspace. install.packages(&quot;tidyverse&quot;) library(&quot;tidyverse&quot;) A package needed to be installed once, but it needs to be (re)loaded in every new session. 1.2.1 Basics 1.2.1.1 mpg dataset mpg is a dataset in tidyverse, which contains observations collected by US Environment Protection Agency on 38 models of cars. mpg To get deatails about mpg run ?mpg 1.2.1.2 Our first ggplot For plotting the relationship between displ and hwy variable in mpg dataset, run this code (It puts the variables in x and y axis respectively): ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) The plots shows negative relationship between Engine Size(displ) and Fuel Efficiency(hwy). Begins with a function ggplot()-creates a coordinate system that you can add addlayers to. The first arugment is the data set to use ggplot(data = mpg) creates an empty graph. Add layers to ggplot()- the function geom_point() adds a layer of points to your plot. Each geom function takes a mapping argument. The mapping argument is always paired with aes() 1.2.1.3 Template for graphing in ggplot2 ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) We will learn in the later subsections how to complete and extend this basic template to make different types of graphs. 1.2.2 Aesthetic mappings in ggplot2 Take a look at the below graph. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class)) We have added a colour = class by mapping it to an aesthetic called colour to for distinct values of class variable. The visual property of the objects you plot is called an aesthetic. It contains the information of size, shape, color of points etc.. By changing the aesthetic properties you can display points in other different ways. 1.2.2.1 aes() in ggplot It Associate the name of the aesthetic to the name of the variable. the function gathers together each of the aesthetic mappings used by a layer and passes them to the layer’s mapping argument. It is clever enough to select a reasonable scale to use with the aesthetic, and it also constructs a legend (or axis labels) that explains the mapping between levels and values. In the above example, the name of the aesthetic was colour and name of the variable is class. ggplot2 smartly assigns a unique level of the aesthetic colour to a unique level to the variable class. This process of unique association is called Scaling. Let’s look at some more examples. The geom allows us to set the aesthetic properties manually (without putting inside aes()). However, it can’t convey information about a variable. It just changes the overall the plot. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), colour = &quot;blue&quot;) As you can see in this example, colour = \"blue\" changes the colour of all points to blue. The alpha aesthetic: It handles the tranparency of points ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, alpha = class)) #&gt; Warning: Using alpha for a discrete variable is not advised. The shape aesthetic: As the name suggests it controls the shape of the points ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, shape = class)) #&gt; Warning: The shape palette can deal with a maximum of 6 discrete values because #&gt; more than 6 becomes difficult to discriminate; you have 7. Consider #&gt; specifying shapes manually if you must have them. #&gt; Warning: Removed 62 rows containing missing values (geom_point). But unfortunately ggplot2 plots only 6 different shapes in a plot! And as a result the additional group suv gets ommited (as you can see in the warning message!). 1.2.3 Facets (wrap and grid). Facets in ggplot splits the plot as per the argument. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class)) + scale_colour_viridis_d(option = &quot;plasma&quot;) + facet_wrap(~class, nrow = 2) with facet_wrap you can split plot by a single variable into subplots that each display one subset of the data. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, colour = class)) + scale_colour_viridis_d(option = &quot;plasma&quot;) + facet_grid(drv ~ cyl) with facet_grid you can split plot by a combination of two variables into subplots that each display one subset of the data. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = class)) + scale_colour_viridis_d(option = &quot;plasma&quot;) + facet_wrap(~class, nrow = 2) 1.2.4 Geometric objects Geometrical object that a plot uses to represent data. These are bar-charts: geom_bar, geom_histogram line-charts: geom_smooth box-plot: geom_boxplot scatter-plot: geom_point 1.2.4.1 Bar-charts: geom_bar First specifies a sequence of points, called breaks. It counts the number of observation between the breaks, called bins. Places a bar in each bin with base being the length of the bin and height determined by either the frequency or proportion of observations in the bin. Look at the following examples: # left ggplot(data = mpg, mapping = aes(x = cty, fill = class)) + geom_histogram() + scale_fill_viridis(discrete = TRUE) # right ggplot(data = mpg, mapping = aes(x = cty, fill = class)) + geom_histogram(binwidth = 5) + scale_fill_viridis(discrete = TRUE) By default ggplot2 picks a suitable binwidth but you can specify it explicitely. table &lt;- as.data.frame(table(mpg$class)) ggplot(data = table) + geom_bar(mapping = aes(x = Var1, stat = &quot;identity&quot;)) #&gt; Warning: Ignoring unknown aesthetics: stat ggplot(data = mpg) + stat_summary( mapping = aes(x = class, y = hwy), fun.min = min, fun.max = max, fun = median ) ggplot(data = mpg) + geom_bar(mapping = aes(x = class, color = class)) ggplot(data = mpg) + stat_count(mapping = aes(x = class)) 1.2.5 Statistical transformations stat-count: Bar charts A bar chart is a graph where for each category a bar with a height proportional to the count in the respective category is drawn. Along x-axis the categories (or levels) are displayed. 1.2.6 Position adjustments 1.2.7 Coordinate systems ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = class)) + scale_fill_viridis_d() + coord_flip() ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = class)) + scale_fill_viridis_d() + coord_polar() ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() ggplot(data = mpg, mapping = aes(x = class, y = hwy)) + geom_boxplot() + coord_flip() ggplot(data = mpg) + geom_bar(mapping = aes(x = class, fill = trans), position = &quot;dodge&quot;) + scale_fill_viridis_d() ggplot( data = filter(mpg, class == &quot;subcompact&quot;), mapping = aes(x = class, y = hwy) ) + geom_boxplot() + coord_flip() 1.2.8 The layered grammar of graphics ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;( mapping = aes(&lt;MAPPINGS&gt;), stat = &lt;STAT&gt;, position = &lt;POSITION&gt; ) + &lt;COORDINATE_FUNCTION&gt; + &lt;FACET_FUNCTION&gt; So far you have learnt some basic data visualization with ggplot2. To extend your data-visualization skills with beautiful and professional plots you can visit The R Graph Gallery website. They have wonderful collection of plots made with ggplot2 and other R tools. Exercises geom_smooth function: After loading library tidyverse execute the following command: ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) Understand (as best as possible) what curve the code is drawing. Add the following aesthetic mappings using variable drv and explain the plot in each case: linetype group linetype colour (use viridis scale filling) Write a R-code that produces one plot in which : there is a scatter plot of displ vs hwy using class for colour (again viridis) and over layered on it a best fit line using geom_smooth for the midsize cars. Go to https://data.incovid19.org and write out a one paragraph description of what the data set contains. Download data set from: https://data.incovid19.org/csv/latest/states.csv . Load state.csv in R into a dataframe called state_df. Pick a state of India which has the same starting letter as the starting letters in your first, middle or last name. For example: Siva Athreya could pick Arunachal Pradesh. Subset the dataframe state_df to have only data from the state that you picked in the previous step and call the resulting dataframe as mystate_df Using mystate_df compute the daily active cases for the state. Then plot a line chart using geom_line for the same from the date you started classs in ,viridis colored by date. Provide: Title as “Active cases for State - name_of_the_state”; x-label - “Dates”, y-label - “Cases”, x-ticks to be dates. Using mystate_df compute the total Deceased figures for each month since March 2020 till date. Then plot a bar chart using geom_bar of the monthly Deceased totals, viridis colored by month. Provide: Title as “Monthly Deceased Totals for State - name_of_the_state”; x-label - “Months”, y-label - “Deceased Total”, x-ticks to be names of months. Using state_df compute the total Confirmed cases and total Deceased for each state since March 2020 till date. Then plot a scatter using geom_point of the total confirmed cases of the states versus total deceased figures; viridis colored by state. Provide: Title as “Scatter Plot of Confirmed Versus Deceased”; x-label - “Deceased Figures”, y-label - “Confirmed Cases”. Can you label the dots with the State names? Examine the structure of the inbuilt dataset iris. How many observations and variables are in the dataset? Using ggplot produce the following scatter plot: "],["1.3-date-and-time-in-r.html", "1.3 Date and Time in R", " 1.3 Date and Time in R There are many options in R to handle Dates and Times. R handles date and time in three classes: - as.Date - POSIXlt - POSIXct POSIX - Portable OS interface on UNIX (allow for date &amp; time without timezone) 2 packages for date-time manipulation - chron - lubricate library(&quot;chron&quot;) library(&quot;lubridate&quot;) #&gt; #&gt; Attaching package: &#39;lubridate&#39; #&gt; The following objects are masked from &#39;package:chron&#39;: #&gt; #&gt; days, hours, minutes, seconds, years #&gt; The following object is masked from &#39;package:cowplot&#39;: #&gt; #&gt; stamp #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; date, intersect, setdiff, union For handling dates without times as.Date() is the best choice as.Date(&quot;1/15/2001&quot;, format = &quot;%m/%d/%Y&quot;) #&gt; [1] &quot;2001-01-15&quot; as.Date(&quot;April 26, 2001&quot;, format = &quot;%B %d, %Y&quot;) #&gt; [1] &quot;2001-04-26&quot; as.Date(&quot;22JUN01&quot;, format = &quot;%d%b%y&quot;) #&gt; [1] &quot;2001-06-22&quot; We have specify format if input dates are not in standard format Code Value %d day of the month (decimal number) %m month (decimal number) %Y Year (4 digits) %B Month (Full name) %y Year (2 digits) %b Month (3 letter abbrebiated name) bdays &lt;- c( CRRao = as.Date(&quot;1920-09-10&quot;), PCMahalanobis = as.Date(&quot;1893-06-29&quot;), Cramer = as.Date(&quot;1893-09-25&quot;), KRParthasarathy = as.Date(&quot;1936-06-25&quot;) ) weekdays() and months() are 2 inbuilt functions in R that provide day of the weekand and name of the month respectively A vector of weekdays extarcted from the elements bdays weekdays(bdays) #&gt; CRRao PCMahalanobis Cramer KRParthasarathy #&gt; &quot;Friday&quot; &quot;Thursday&quot; &quot;Monday&quot; &quot;Thursday&quot; A vector of months extarcted from the elements bdays months(bdays) #&gt; CRRao PCMahalanobis Cramer KRParthasarathy #&gt; &quot;September&quot; &quot;June&quot; &quot;September&quot; &quot;June&quot; For computation with dates we must specify format of date datef1 &lt;- as.Date(&quot;02/08/2021&quot;, format = &quot;%m/%d/%Y&quot;) datef1 #&gt; [1] &quot;2021-02-08&quot; datef2 &lt;- as.Date(&quot;February 8, 2021&quot;, format = &quot;%B %d, %Y&quot;) datef2 #&gt; [1] &quot;2021-02-08&quot; datef &lt;- as.Date(&quot;04/08/2021&quot;, format = &quot;%m/%d/%Y&quot;) datef2 &lt;- as.Date(&quot;October 8, 2021&quot;, format = &quot;%B %d, %Y&quot;) Subtraction - specifies difference in days datef1 - datef2 #&gt; Time difference of -242 days Built-in function difftime() - specifies the diff in specified units difftime(datef1, datef2, units = &quot;weeks&quot;) #&gt; Time difference of -34.57143 weeks difftime(datef1, datef2, units = &quot;days&quot;) #&gt; Time difference of -242 days difftime(datef1, datef2) #&gt; Time difference of -242 days datef1 #&gt; [1] &quot;2021-02-08&quot; datef2 #&gt; [1] &quot;2021-10-08&quot; Add and subract days form dates datef2 + 10 #&gt; [1] &quot;2021-10-18&quot; datef1 - 10 #&gt; [1] &quot;2021-01-29&quot; Vector of dates three.days &lt;- as.Date(c(&quot;2020-07-22&quot;, &quot;2019-04-20&quot;, &quot;2022-10-06&quot;)) three.days #&gt; [1] &quot;2020-07-22&quot; &quot;2019-04-20&quot; &quot;2022-10-06&quot; interval differences between dates diff(three.days) #&gt; Time differences in days #&gt; [1] -459 1265 Produce 7 dates that differ by a week, starting from datef1 Seven &lt;- seq(datef1, length = 7, by = &quot;week&quot;) Seven #&gt; [1] &quot;2021-02-08&quot; &quot;2021-02-15&quot; &quot;2021-02-22&quot; &quot;2021-03-01&quot; &quot;2021-03-08&quot; #&gt; [6] &quot;2021-03-15&quot; &quot;2021-03-22&quot; Produce 7 dates that differ by a 14 days, starting from datef1 Seven &lt;- seq(datef1, length = 7, by = 14) Seven #&gt; [1] &quot;2021-02-08&quot; &quot;2021-02-22&quot; &quot;2021-03-08&quot; &quot;2021-03-22&quot; &quot;2021-04-05&quot; #&gt; [6] &quot;2021-04-19&quot; &quot;2021-05-03&quot; Produce 7 dates that differ by a 2 weeks (same as prev one), starting from datef1 Seven &lt;- seq(datef1, length = 7, by = &quot;2 weeks&quot;) Seven #&gt; [1] &quot;2021-02-08&quot; &quot;2021-02-22&quot; &quot;2021-03-08&quot; &quot;2021-03-22&quot; &quot;2021-04-05&quot; #&gt; [6] &quot;2021-04-19&quot; &quot;2021-05-03&quot; It is conjunction with a package - stringr — very useful Combine dates &amp; times with string manipulation ?strptime POSIX- Portable OS interface on UNIX (allow for dat &amp; time without timezone) chron - (allows for dates &amp; time) as.POSIXct() Handles dates and times also takes care of time zones accurate representation of time Time1 &lt;- as.POSIXct(x = &quot;2023-07-24 23:55:26&quot;) Time1 #&gt; [1] &quot;2023-07-24 23:55:26 UTC&quot; Time2 &lt;- as.POSIXct(&quot;25072023 08:32:07&quot;, format = &quot;%d%m%Y %H:%M:%S&quot;) Time2 #&gt; [1] &quot;2023-07-25 08:32:07 UTC&quot; Specifying timezone Time3 &lt;- as.POSIXct(&quot;2020-01-01 11:42:03&quot;, tz = &quot;GMT&quot;) Time3 &lt;- as.POSIXct(&quot;2020-01-01 11:42:03&quot;, tz = Sys.timezone()) Time3 #&gt; [1] &quot;2020-01-01 11:42:03 UTC&quot; To get timezone of the system Sys.timezone() #&gt; [1] &quot;UTC&quot; Operations with times Time2 &gt; Time1 #&gt; [1] TRUE Time difference Time2 - Time1 #&gt; Time difference of 8.611389 hours adds 30s Time1 + 30 #&gt; [1] &quot;2023-07-24 23:55:56 UTC&quot; subtract 30s Time1 - 30 #&gt; [1] &quot;2023-07-24 23:54:56 UTC&quot; Use as. to do computations Adjust for daylight savings time as.POSIXct(&quot;2021-03-10 08:32:07&quot;) - as.POSIXct(&quot;2023-03-09 23:55:26&quot;) #&gt; Time difference of -729.6412 days Sys.time() #&gt; [1] &quot;2022-07-28 08:07:00 UTC&quot; Internal integer representstion unclass(Time1) #&gt; [1] 1690242926 #&gt; attr(,&quot;tzone&quot;) #&gt; [1] &quot;&quot; difference from 1970-01-01 00:00:00 UTC difftime(Time1, as.POSIXct(&quot;1970-01-01 00:00:00&quot;, tz = &quot;UTC&quot;), units = &quot;secs&quot;) #&gt; Time difference of 1690242926 secs lt - Local time ct - Calender time Time1.lt &lt;- as.POSIXlt(&quot;2022-07-24 23:55:26&quot;) Time1.lt #&gt; [1] &quot;2022-07-24 23:55:26 UTC&quot; These 2 functions are used to strip specific units from date &amp; time in column form unclass(Time1.lt) #&gt; $sec #&gt; [1] 26 #&gt; #&gt; $min #&gt; [1] 55 #&gt; #&gt; $hour #&gt; [1] 23 #&gt; #&gt; $mday #&gt; [1] 24 #&gt; #&gt; $mon #&gt; [1] 6 #&gt; #&gt; $year #&gt; [1] 122 #&gt; #&gt; $wday #&gt; [1] 0 #&gt; #&gt; $yday #&gt; [1] 204 #&gt; #&gt; $isdst #&gt; [1] 0 #&gt; #&gt; attr(,&quot;tzone&quot;) #&gt; [1] &quot;UTC&quot; in list form unlist(Time1.lt) #&gt; sec min hour mday mon year wday yday isdst #&gt; 26 55 23 24 6 122 0 204 0 getting seconds of Time1.lt Time1.lt$sec #&gt; [1] 26 getting corresponding weekday number of Time1.lt Time1.lt$wday #&gt; [1] 0 date &amp; time till the day trunc(Time1.lt, &quot;days&quot;) #&gt; [1] &quot;2022-07-24 UTC&quot; date &amp; time till the minuites trunc(Time1.lt, &quot;mins&quot;) #&gt; [1] &quot;2022-07-24 23:55:00 UTC&quot; The chron R package require(chron) Creating times in chron time1.c &lt;- as.chron(&quot;2013-07-24 23:55:26&quot;) Gives time in chron format time1.c #&gt; [1] (07/24/13 23:55:26) time2.c &lt;- as.chron(&quot;07/25/13&quot;, &quot;%m/%d/%Y&quot;) Gives date in the specified format time2.c #&gt; [1] (07/25/13 00:00:00) Extracting the date with date() dates(time1.c) #&gt; day #&gt; 07/24/13 Operaions with time variable logical statement comparing time time2.c &gt; time1.c #&gt; [1] FALSE Adding 10 days time1.c + 10 #&gt; [1] (08/03/13 23:55:26) Subtraction of time vaiable time2.c - time1.c #&gt; [1] -730485 Gives difference in the unit specified hours difftime(time2.c, time1.c, unit = &quot;hours&quot;) #&gt; Time difference of -17531640 hours Gives difference in the time as.chron(&quot;2013-03-10 08:32:07&quot;) - as.chron(&quot;2013-03-09 23:55:26&quot;) #&gt; [1] 08:36:41 NOTE: Chron does not adjust for the time zones Exercises What is the significance of January 1, 1970 ? What is the difference between as.Date and POSIXlt ? "],["1.4-factors-and-levels-in-r.html", "1.4 Factors and Levels in R", " 1.4 Factors and Levels in R In data analysis many times we deal with categorical data or variables which may be Boolean TRUE/FALSE or types of cars or gender in other words Qualitative variables. These cannot be meaningfully expressed in numbers. But how to handle them in R? The answer is factors and levels. They are often useful for Statistical Modeling and Plotting data. You can work with factors and levels in base-R. But some tidyverse packages: dplyr, tidyr, forcats, readr also help to deal with them. factor(x) - Store as vector of integers - Displayed as characters x_char &lt;- c( &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;August&quot;, &quot;July&quot;, &quot;July&quot;, &quot;August&quot; ) create a vector of data factor_x_char &lt;- factor(x_char) store the vector as a factor using factor() factor_x_char #&gt; [1] June July August September August July July #&gt; [8] August #&gt; Levels: August July June September factors in R are stored as a vector of integers but they correspond to a character string for display Levels are the unique set of values taken by as.character() [this is the default assignment] factor levels are always characters x_num - numeric data as factors x_num &lt;- c(1, 2, 2, 3, 1, 2, 3, 3, 1, 2, 3, 3, 1) Create a vector of data factor_x_num &lt;- factor(x_num) Store it as a factor using factor() factor_x_num #&gt; [1] 1 2 2 3 1 2 3 3 1 2 3 3 1 #&gt; Levels: 1 2 3 Note: factor levels are always characters Both numeric and character data can be stored as factors mean(x_num) #&gt; [1] 2.076923 But, factor levels are always characters mean(factor_x_num) #&gt; Warning in mean.default(factor_x_num): argument is not numeric or logical: #&gt; returning NA #&gt; [1] NA Since, argument is not numeric, so will return NA use levels() function to convert them to original numeric value mean(as.numeric(levels(factor_x_num)[factor_x_num])) #&gt; [1] 2.076923 x_char - character data as factors x_char &lt;- c( &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;August&quot;, &quot;July&quot;, &quot;July&quot;, &quot;August&quot; ) factor_x_char &lt;- factor(x_char) factor_x_char #&gt; [1] June July August September August July July #&gt; [8] August #&gt; Levels: August July June September table(factor_x_char) #&gt; factor_x_char #&gt; August July June September #&gt; 3 3 1 1 table() gives frequency of each level ordering of a is with respoect to as.character() and not related to the order in the vector. months &lt;- factor(x_char, levels = c( &quot;Garbage&quot;, &quot;January&quot;, &quot;February&quot;, &quot;March&quot;, &quot;April&quot;, &quot;May&quot;, &quot;June&quot;, &quot;July&quot;, &quot;August&quot;, &quot;September&quot;, &quot;October&quot;, &quot;November&quot;, &quot;December&quot; ), ordered = TRUE ) we can specify the levels and the order that we want months #&gt; [1] June July August September August July July #&gt; [8] August #&gt; 13 Levels: Garbage &lt; January &lt; February &lt; March &lt; April &lt; May &lt; ... &lt; December Here, in our specification, levels contain elements not present in months months[3] &lt; months[4] #&gt; [1] TRUE When factor is created, all it’s levels are stored with the factor table(months) #&gt; months #&gt; Garbage January February March April May June July #&gt; 0 0 0 0 0 0 1 3 #&gt; August September October November December #&gt; 3 1 0 0 0 table() will display ALL the levels specified and respective counts occur when subsetting a factor factor(months) #&gt; [1] June July August September August July July #&gt; [8] August #&gt; Levels: June &lt; July &lt; August &lt; September retains only the levels present without affecting the ordering (i.e and maintains their order.) table(factor(months)) #&gt; #&gt; June July August September #&gt; 1 3 3 1 Gives the count of ONLY the levels present in the factor 1.4.0.0.1 x &lt;- round(1000 * runif(10)) x #&gt; [1] 185 702 573 168 944 943 129 833 468 550 cut() function is used to convert a numeric variable into a factor Syntax: cut(numeric_data, breaks) xfactor &lt;- cut(x, 4) breaks - arbitary by default - specifies how range of numbers will be converted to factor values xfactor #&gt; [1] (128,333] (536,740] (536,740] (128,333] (740,945] (740,945] (128,333] #&gt; [8] (740,945] (333,536] (536,740] #&gt; Levels: (128,333] (333,536] (536,740] (740,945] table(xfactor) #&gt; xfactor #&gt; (128,333] (333,536] (536,740] (740,945] #&gt; 3 1 3 3 displays the count of each level that was created using cut() cut(x, 3, labels = c(&quot;L&quot;, &quot;M&quot;, &quot;H&quot;)) #&gt; [1] L H M L H H L H M M #&gt; Levels: L M H label() - used to specify the levels of the factors Nicer set of labels with pretty() xpfactor &lt;- cut(x, pretty(x, 4)) xpfactor #&gt; [1] (0,200] (600,800] (400,600] (0,200] (800,1e+03] (800,1e+03] #&gt; [7] (0,200] (800,1e+03] (400,600] (400,600] #&gt; Levels: (0,200] (200,400] (400,600] (600,800] (800,1e+03] but may or may not provide more levels than specified table(xpfactor) #&gt; xpfactor #&gt; (0,200] (200,400] (400,600] (600,800] (800,1e+03] #&gt; 3 0 3 1 3 displays the count of each level created with pretty() xqfactor &lt;- cut(x, quantile(x, probs = seq(0, 1, 0.25))) Produce factors based on percentiles of your data table(xqfactor) #&gt; xqfactor #&gt; (129,256] (256,562] (562,800] (800,944] #&gt; 2 2 2 3 observations are distributed “equally” in each level everyday &lt;- seq( from = as.Date(&quot;2021-1-1&quot;), to = as.Date(&quot;2021-12-31&quot;), by = &quot;day&quot; ) Create factors from dates/times strptime(), strftime() with factors and levels would be very useful for extracting information with ordering from a scraped dataset. cmonth &lt;- format(everyday, &quot;%b&quot;) format() can be used to extract the month from each date using ‘%b’ Extract month from each day head(cmonth, 3) #&gt; [1] &quot;Jan&quot; &quot;Jan&quot; &quot;Jan&quot; df &lt;- as.data.frame(table(cmonth)) table() - tabulates values in each month - ordering in alphabetcal as.data.frame() stores the output as dataframe names(df) &lt;- c(&quot;Month&quot;, &quot;Freq&quot;) changing the names of the variables of the dataframe d df months &lt;- factor(cmonth, levels = unique(cmonth), ordered = TRUE ) unique(): returns the unique values in the order that are encountered stored months as factors df2 &lt;- as.data.frame(table(months)) names(df2) &lt;- c(&quot;Month&quot;, &quot;Freq&quot;) specified levels &amp; ordering using the unique() df2 #&gt; Month Freq #&gt; 1 Jan 31 #&gt; 2 Feb 28 #&gt; 3 Mar 31 #&gt; 4 Apr 30 #&gt; 5 May 31 #&gt; 6 Jun 30 #&gt; 7 Jul 31 #&gt; 8 Aug 31 #&gt; 9 Sep 30 #&gt; 10 Oct 31 #&gt; 11 Nov 30 #&gt; 12 Dec 31 everyday &lt;- seq( from = as.Date(&quot;2021-1-1&quot;), to = as.Date(&quot;2021-12-31&quot;), by = &quot;day&quot; ) vector of all Dates of the year 2021 wks &lt;- cut(everyday, breaks = &quot;week&quot;) levels - 53 dates of each week of the year 2021 wks #&gt; [1] 2020-12-28 2020-12-28 2020-12-28 2021-01-04 2021-01-04 2021-01-04 #&gt; [7] 2021-01-04 2021-01-04 2021-01-04 2021-01-04 2021-01-11 2021-01-11 #&gt; [13] 2021-01-11 2021-01-11 2021-01-11 2021-01-11 2021-01-11 2021-01-18 #&gt; [19] 2021-01-18 2021-01-18 2021-01-18 2021-01-18 2021-01-18 2021-01-18 #&gt; [25] 2021-01-25 2021-01-25 2021-01-25 2021-01-25 2021-01-25 2021-01-25 #&gt; [31] 2021-01-25 2021-02-01 2021-02-01 2021-02-01 2021-02-01 2021-02-01 #&gt; [37] 2021-02-01 2021-02-01 2021-02-08 2021-02-08 2021-02-08 2021-02-08 #&gt; [43] 2021-02-08 2021-02-08 2021-02-08 2021-02-15 2021-02-15 2021-02-15 #&gt; [49] 2021-02-15 2021-02-15 2021-02-15 2021-02-15 2021-02-22 2021-02-22 #&gt; [55] 2021-02-22 2021-02-22 2021-02-22 2021-02-22 2021-02-22 2021-03-01 #&gt; [61] 2021-03-01 2021-03-01 2021-03-01 2021-03-01 2021-03-01 2021-03-01 #&gt; [67] 2021-03-08 2021-03-08 2021-03-08 2021-03-08 2021-03-08 2021-03-08 #&gt; [73] 2021-03-08 2021-03-15 2021-03-15 2021-03-15 2021-03-15 2021-03-15 #&gt; [79] 2021-03-15 2021-03-15 2021-03-22 2021-03-22 2021-03-22 2021-03-22 #&gt; [85] 2021-03-22 2021-03-22 2021-03-22 2021-03-29 2021-03-29 2021-03-29 #&gt; [91] 2021-03-29 2021-03-29 2021-03-29 2021-03-29 2021-04-05 2021-04-05 #&gt; [97] 2021-04-05 2021-04-05 2021-04-05 2021-04-05 2021-04-05 2021-04-12 #&gt; [103] 2021-04-12 2021-04-12 2021-04-12 2021-04-12 2021-04-12 2021-04-12 #&gt; [109] 2021-04-19 2021-04-19 2021-04-19 2021-04-19 2021-04-19 2021-04-19 #&gt; [115] 2021-04-19 2021-04-26 2021-04-26 2021-04-26 2021-04-26 2021-04-26 #&gt; [121] 2021-04-26 2021-04-26 2021-05-03 2021-05-03 2021-05-03 2021-05-03 #&gt; [127] 2021-05-03 2021-05-03 2021-05-03 2021-05-10 2021-05-10 2021-05-10 #&gt; [133] 2021-05-10 2021-05-10 2021-05-10 2021-05-10 2021-05-17 2021-05-17 #&gt; [139] 2021-05-17 2021-05-17 2021-05-17 2021-05-17 2021-05-17 2021-05-24 #&gt; [145] 2021-05-24 2021-05-24 2021-05-24 2021-05-24 2021-05-24 2021-05-24 #&gt; [151] 2021-05-31 2021-05-31 2021-05-31 2021-05-31 2021-05-31 2021-05-31 #&gt; [157] 2021-05-31 2021-06-07 2021-06-07 2021-06-07 2021-06-07 2021-06-07 #&gt; [163] 2021-06-07 2021-06-07 2021-06-14 2021-06-14 2021-06-14 2021-06-14 #&gt; [169] 2021-06-14 2021-06-14 2021-06-14 2021-06-21 2021-06-21 2021-06-21 #&gt; [175] 2021-06-21 2021-06-21 2021-06-21 2021-06-21 2021-06-28 2021-06-28 #&gt; [181] 2021-06-28 2021-06-28 2021-06-28 2021-06-28 2021-06-28 2021-07-05 #&gt; [187] 2021-07-05 2021-07-05 2021-07-05 2021-07-05 2021-07-05 2021-07-05 #&gt; [193] 2021-07-12 2021-07-12 2021-07-12 2021-07-12 2021-07-12 2021-07-12 #&gt; [199] 2021-07-12 2021-07-19 2021-07-19 2021-07-19 2021-07-19 2021-07-19 #&gt; [205] 2021-07-19 2021-07-19 2021-07-26 2021-07-26 2021-07-26 2021-07-26 #&gt; [211] 2021-07-26 2021-07-26 2021-07-26 2021-08-02 2021-08-02 2021-08-02 #&gt; [217] 2021-08-02 2021-08-02 2021-08-02 2021-08-02 2021-08-09 2021-08-09 #&gt; [223] 2021-08-09 2021-08-09 2021-08-09 2021-08-09 2021-08-09 2021-08-16 #&gt; [229] 2021-08-16 2021-08-16 2021-08-16 2021-08-16 2021-08-16 2021-08-16 #&gt; [235] 2021-08-23 2021-08-23 2021-08-23 2021-08-23 2021-08-23 2021-08-23 #&gt; [241] 2021-08-23 2021-08-30 2021-08-30 2021-08-30 2021-08-30 2021-08-30 #&gt; [247] 2021-08-30 2021-08-30 2021-09-06 2021-09-06 2021-09-06 2021-09-06 #&gt; [253] 2021-09-06 2021-09-06 2021-09-06 2021-09-13 2021-09-13 2021-09-13 #&gt; [259] 2021-09-13 2021-09-13 2021-09-13 2021-09-13 2021-09-20 2021-09-20 #&gt; [265] 2021-09-20 2021-09-20 2021-09-20 2021-09-20 2021-09-20 2021-09-27 #&gt; [271] 2021-09-27 2021-09-27 2021-09-27 2021-09-27 2021-09-27 2021-09-27 #&gt; [277] 2021-10-04 2021-10-04 2021-10-04 2021-10-04 2021-10-04 2021-10-04 #&gt; [283] 2021-10-04 2021-10-11 2021-10-11 2021-10-11 2021-10-11 2021-10-11 #&gt; [289] 2021-10-11 2021-10-11 2021-10-18 2021-10-18 2021-10-18 2021-10-18 #&gt; [295] 2021-10-18 2021-10-18 2021-10-18 2021-10-25 2021-10-25 2021-10-25 #&gt; [301] 2021-10-25 2021-10-25 2021-10-25 2021-10-25 2021-11-01 2021-11-01 #&gt; [307] 2021-11-01 2021-11-01 2021-11-01 2021-11-01 2021-11-01 2021-11-08 #&gt; [313] 2021-11-08 2021-11-08 2021-11-08 2021-11-08 2021-11-08 2021-11-08 #&gt; [319] 2021-11-15 2021-11-15 2021-11-15 2021-11-15 2021-11-15 2021-11-15 #&gt; [325] 2021-11-15 2021-11-22 2021-11-22 2021-11-22 2021-11-22 2021-11-22 #&gt; [331] 2021-11-22 2021-11-22 2021-11-29 2021-11-29 2021-11-29 2021-11-29 #&gt; [337] 2021-11-29 2021-11-29 2021-11-29 2021-12-06 2021-12-06 2021-12-06 #&gt; [343] 2021-12-06 2021-12-06 2021-12-06 2021-12-06 2021-12-13 2021-12-13 #&gt; [349] 2021-12-13 2021-12-13 2021-12-13 2021-12-13 2021-12-13 2021-12-20 #&gt; [355] 2021-12-20 2021-12-20 2021-12-20 2021-12-20 2021-12-20 2021-12-20 #&gt; [361] 2021-12-27 2021-12-27 2021-12-27 2021-12-27 2021-12-27 #&gt; 53 Levels: 2020-12-28 2021-01-04 2021-01-11 2021-01-18 ... 2021-12-27 qtrs &lt;- cut(everyday, breaks = &quot;3 months&quot;, labels = paste(&quot;Q&quot;, 1:4, sep = &quot;&quot;) ) levels - 4 quarters of the year 2021 qtrs #&gt; [1] Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 #&gt; [26] Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 #&gt; [51] Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 #&gt; [76] Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q1 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 #&gt; [101] Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 #&gt; [126] Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 #&gt; [151] Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 Q2 #&gt; [176] Q2 Q2 Q2 Q2 Q2 Q2 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 #&gt; [201] Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 #&gt; [226] Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 #&gt; [251] Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q3 Q4 Q4 #&gt; [276] Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 #&gt; [301] Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 #&gt; [326] Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 #&gt; [351] Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 Q4 #&gt; Levels: Q1 Q2 Q3 Q4 decdf &lt;- read.csv( file = &quot;https://www.isibang.ac.in/~athreya/Teaching/ISCD/Master.csv&quot;, header = TRUE ) This is the deceased data from the Government of Karnataka COVID-19 Bulletin. The https://www.isibang.ac.in/~athreya/Teaching/ISCD/Master.csv file contains this collation. decdf$Month &lt;- months(as.Date(decdf$MB.Date)) Use months() to extract months of reporting date data &lt;- as.data.frame(table(decdf$Month)) names(data) &lt;- c(&quot;name&quot;, &quot;val&quot;) Use table() to compute reported cases across months change names of dataframe data #&gt; name val #&gt; 1 April 2974 #&gt; 2 August 4108 #&gt; 3 December 430 #&gt; 4 February 483 #&gt; 5 January 843 #&gt; 6 July 3561 #&gt; 7 June 6049 #&gt; 8 March 239 #&gt; 9 May 13599 #&gt; 10 November 737 #&gt; 11 October 2593 #&gt; 12 September 3643 ggplot(data = data, aes(x = name, y = val, fill = name)) + geom_bar(stat = &quot;identity&quot;, alpha = .6, width = .4) + coord_flip() + scale_fill_viridis_d() + xlab(&quot;&quot;) + theme_bw() Note: - ggplot uses ordering alphabetically - treats Month as factor and default level ordering try to plot below in order of frequency values. decdf &lt;- read.csv( file = &quot;https://www.isibang.ac.in/~athreya/Teaching/ISCD/Master.csv&quot;, header = TRUE ) decdf$Month &lt;- months(as.Date(decdf$MB.Date)) data &lt;- as.data.frame(table(decdf$Month)) names(data) &lt;- c(&quot;name&quot;, &quot;val&quot;) create data frame data &lt;- arrange(data, val) Note: We arranged our dataframe to preferred ordering. ggplot(data = data, aes(x = name, y = val, fill = name)) + geom_bar(stat = &quot;identity&quot;, alpha = .6, width = .4) + scale_fill_viridis_d() + coord_flip() + xlab(&quot;&quot;) + theme_bw() ggplot() takes into account ordering from the factor given by its levels &amp; NOT as we see in the data-frame Inspite of arranging our dataframe, the plot is still the same as before! Same code as before - some correction to get required result try to plot below in order of frequency values. library(tidyverse) decdf &lt;- read.csv( file = &quot;https://www.isibang.ac.in/~athreya/Teaching/ISCD/Master.csv&quot;, header = TRUE ) decdf$Month &lt;- months(as.Date(decdf$MB.Date)) data &lt;- as.data.frame(table(decdf$Month)) names(data) &lt;- c(&quot;name&quot;, &quot;val&quot;) data &lt;- arrange(data, val) data$name &lt;- factor(data$name, levels = data$name) Reorder the levels according to the values ggplot(data = data, aes(x = name, y = val, fill = name)) + geom_bar(stat = &quot;identity&quot;, alpha = .6, width = .4) + coord_flip() + scale_fill_viridis_d() + xlab(&quot;&quot;) + theme_bw() ggplot() obliges the order now using dplyr and forcats decdf &lt;- read.csv( file = &quot;https://www.isibang.ac.in/~athreya/Teaching/ISCD/Master.csv&quot;, header = TRUE ) decdf$Month &lt;- months(as.Date(decdf$MB.Date)) create data frame library(tidyverse) ggplot( data = decdf, mapping = aes( x = fct_infreq(Month), fill = cut(Age.In.Years, pretty(Age.In.Years, 4)) ) ) + geom_bar(stat = &quot;count&quot;, alpha = .6, width = .4) + scale_fill_viridis_d() + xlab(&quot;&quot;) + coord_flip() + theme_bw() With dplyr(R package): Order months according to reported deaths With forcats(R package): fct_infreq library(dplyr) library(forcats) ggplot( data = decdf, mapping = aes( x = fct_infreq(Month), y = cut(Age.In.Years, pretty(Age.In.Years, 4)) # cut() is used to add Age.In.Years bin layer ) ) + geom_bar(stat = &quot;identity&quot;, width = .4) + labs(fill = &quot;Age&quot;) + scale_fill_viridis_d() + coord_flip() + ggtitle(&quot;Deceased Month Count- Age Stacked&quot;) + ylab(&quot;Count of Deceased&quot;) + xlab(&quot;Month&quot;) + theme_bw() + theme( plot.title = element_text( color = &quot;#e95462&quot;, size = 14, face = &quot;bold&quot;, hjust = 0.5 ), axis.title.x = element_text( color = &quot;#e95462&quot;, size = 14, vjust = 0.5, face = &quot;bold&quot; ), axis.title.y = element_text( color = &quot;#e95462&quot;, size = 14, face = &quot;bold&quot; ) ) forcats used to create ordered factors. fill command used to add age as layer. labeling of axis, title and ticks discussed above. check other themes in ggplot cut()from base R was used to add Age bin layer Viridis hex_charodes from https://waldyrious.net/viridis-palette-generator/ Exercises # Fucntion definition syntax in R function_name &lt;- function(comma_separated_parameters) { function_statements .. .. } Write functions using the above syantax in R that do each the following: [Decide appropriately what the input and outputs should be. Each item must be separate function that can be called.] Find the levels of factor of a given vector. Change the first level of a factor with another level of a given factor. Create an ordered factor from data consisting of the names of months. Concatenate two given factors in a single factor. Convert a given vector of integers to an ordered factor. Extract the five of the levels of factor created from a random sample from the LETTERS. Create a factor corresponding to height in women data set, which contains height and weights for a sample of women. "],["1.5-data.html", "1.5 Data", " 1.5 Data Test-Change Statistics is based on data and it’s analysis which has a rich and wide literature. 1.5.1 Types of Data We generally deal with 3 kinds of data in statistics: Discrete Numeric Data Categorical Data Continuous Numeric Data This paper gives a broad classification of data from measurements into 9 categories. 1.5.2 Discrete Numeric Data Sometimes we see that many data are described in terms of numbers and many variables naturally take only discrete values. Such data can be visualized with Boxplot and Histograms. Some imporatnt and key features of Discrete Numeric Data are Centre, Spread and the Spread. Center Widely used measure of centre is the mean or the average of the data set. Other measures include the median and the mode . They tell us where the data is centered around. For example, if we have a dataset of 10 numbers (Say, 1, 90, 48, 7, 7, 8, 9, 2, 3, 4) and order them by lowest to highest (i.e., 1, 2, 3, 4, 7, 7, 8, 9, 48, 90) and if we change the largest one by a larger number and smallest one by a smaller number the mean, median, mode may not change but if we change only the smallest one, then the mean will change but median and mode will not. Spread Understanding variabiity of the given data is very important. If one were to understand mean as specifying the center then the range of the data set around it is determined by its variability or spread. It is often measured by the variance(var) or standard deviation(sd) or the inter-quartile range(IQR). For example, Suppose, we have a dataset of Statistics exam score where everyone does well and get scores 98, 99, 100 then the spread of the data is low. But in the same exam if some students get 0, 4, 10 and some students get 90, 92 then the spread is high. Shape To understand various distributional aspects of the dataset one needs to understand its shape. For example, if it is symmetric or skewed round it’s mean. Other aspects include among the data points which are more likely than others. For example, Suppose, we take the probability mass function of \\(Bin(10, \\frac{1}{2})\\) Then we know it’s shape is symmetric about \\(x = \\frac{1}{2}\\). But if we take the density of \\(Poi(4)\\). We will notice that it is not symmetric. The shape of the distribution is governed by the nature of it’s graph around the mean, wheather it is skewed left or right. 1.5.3 Playing with Datasets in R R has a lot inbuilt Datasets that one can use. The command data() will list currently installed data sets. Many datasets in R are often stored as data frame (data.frame). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). Let us learn about a real dataset airquality stored as data frame. Use help to know about it. ?airquality You can print entire data set on the screen by calling it airquality but it prints entire dataset which may be large most of the times! Let us try the head() function head(airquality) #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8.0 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 5 NA NA 14.3 56 5 5 #&gt; 6 28 NA 14.9 66 5 6 This provides the first six rows. Now try the tail() function tail(airquality) #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 148 14 20 16.6 63 9 25 #&gt; 149 30 193 6.9 70 9 26 #&gt; 150 NA 145 13.2 77 9 27 #&gt; 151 14 191 14.3 75 9 28 #&gt; 152 18 131 8.0 76 9 29 #&gt; 153 20 223 11.5 68 9 30 This provides the last six rows. But sometimes we may want to print more or less rows. head() and tail() both functions has a parameter n with which we can print our desired number of rows. head(airquality, n = 10) #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 1 41 190 7.4 67 5 1 #&gt; 2 36 118 8.0 72 5 2 #&gt; 3 12 149 12.6 74 5 3 #&gt; 4 18 313 11.5 62 5 4 #&gt; 5 NA NA 14.3 56 5 5 #&gt; 6 28 NA 14.9 66 5 6 #&gt; 7 23 299 8.6 65 5 7 #&gt; 8 19 99 13.8 59 5 8 #&gt; 9 8 19 20.1 61 5 9 #&gt; 10 NA 194 8.6 69 5 10 A specific datapoint of a data frame can be called using row and column number airquality[148, 4] #&gt; [1] 63 The same can be printed by using the variable name for the given column and call it by it’s position. airquality$Temp[148] #&gt; [1] 63 An entire row can be printed like this. airquality[148, ] #&gt; Ozone Solar.R Wind Temp Month Day #&gt; 148 14 20 16.6 63 9 25 We can pass vector of Provides Ozone and Temp columns airquality[, c(1, 4)] #&gt; Ozone Temp #&gt; 1 41 67 #&gt; 2 36 72 #&gt; 3 12 74 #&gt; 4 18 62 #&gt; 5 NA 56 #&gt; 6 28 66 #&gt; 7 23 65 #&gt; 8 19 59 #&gt; 9 8 61 #&gt; 10 NA 69 #&gt; 11 7 74 #&gt; 12 16 69 #&gt; 13 11 66 #&gt; 14 14 68 #&gt; 15 18 58 #&gt; 16 14 64 #&gt; 17 34 66 #&gt; 18 6 57 #&gt; 19 30 68 #&gt; 20 11 62 #&gt; 21 1 59 #&gt; 22 11 73 #&gt; 23 4 61 #&gt; 24 32 61 #&gt; 25 NA 57 #&gt; 26 NA 58 #&gt; 27 NA 57 #&gt; 28 23 67 #&gt; 29 45 81 #&gt; 30 115 79 #&gt; 31 37 76 #&gt; 32 NA 78 #&gt; 33 NA 74 #&gt; 34 NA 67 #&gt; 35 NA 84 #&gt; 36 NA 85 #&gt; 37 NA 79 #&gt; 38 29 82 #&gt; 39 NA 87 #&gt; 40 71 90 #&gt; 41 39 87 #&gt; 42 NA 93 #&gt; 43 NA 92 #&gt; 44 23 82 #&gt; 45 NA 80 #&gt; 46 NA 79 #&gt; 47 21 77 #&gt; 48 37 72 #&gt; 49 20 65 #&gt; 50 12 73 #&gt; 51 13 76 #&gt; 52 NA 77 #&gt; 53 NA 76 #&gt; 54 NA 76 #&gt; 55 NA 76 #&gt; 56 NA 75 #&gt; 57 NA 78 #&gt; 58 NA 73 #&gt; 59 NA 80 #&gt; 60 NA 77 #&gt; 61 NA 83 #&gt; 62 135 84 #&gt; 63 49 85 #&gt; 64 32 81 #&gt; 65 NA 84 #&gt; 66 64 83 #&gt; 67 40 83 #&gt; 68 77 88 #&gt; 69 97 92 #&gt; 70 97 92 #&gt; 71 85 89 #&gt; 72 NA 82 #&gt; 73 10 73 #&gt; 74 27 81 #&gt; 75 NA 91 #&gt; 76 7 80 #&gt; 77 48 81 #&gt; 78 35 82 #&gt; 79 61 84 #&gt; 80 79 87 #&gt; 81 63 85 #&gt; 82 16 74 #&gt; 83 NA 81 #&gt; 84 NA 82 #&gt; 85 80 86 #&gt; 86 108 85 #&gt; 87 20 82 #&gt; 88 52 86 #&gt; 89 82 88 #&gt; 90 50 86 #&gt; 91 64 83 #&gt; 92 59 81 #&gt; 93 39 81 #&gt; 94 9 81 #&gt; 95 16 82 #&gt; 96 78 86 #&gt; 97 35 85 #&gt; 98 66 87 #&gt; 99 122 89 #&gt; 100 89 90 #&gt; 101 110 90 #&gt; 102 NA 92 #&gt; 103 NA 86 #&gt; 104 44 86 #&gt; 105 28 82 #&gt; 106 65 80 #&gt; 107 NA 79 #&gt; 108 22 77 #&gt; 109 59 79 #&gt; 110 23 76 #&gt; 111 31 78 #&gt; 112 44 78 #&gt; 113 21 77 #&gt; 114 9 72 #&gt; 115 NA 75 #&gt; 116 45 79 #&gt; 117 168 81 #&gt; 118 73 86 #&gt; 119 NA 88 #&gt; 120 76 97 #&gt; 121 118 94 #&gt; 122 84 96 #&gt; 123 85 94 #&gt; 124 96 91 #&gt; 125 78 92 #&gt; 126 73 93 #&gt; 127 91 93 #&gt; 128 47 87 #&gt; 129 32 84 #&gt; 130 20 80 #&gt; 131 23 78 #&gt; 132 21 75 #&gt; 133 24 73 #&gt; 134 44 81 #&gt; 135 21 76 #&gt; 136 28 77 #&gt; 137 9 71 #&gt; 138 13 71 #&gt; 139 46 78 #&gt; 140 18 67 #&gt; 141 13 76 #&gt; 142 24 68 #&gt; 143 16 82 #&gt; 144 13 64 #&gt; 145 23 71 #&gt; 146 36 81 #&gt; 147 7 69 #&gt; 148 14 63 #&gt; 149 30 70 #&gt; 150 NA 77 #&gt; 151 14 75 #&gt; 152 18 76 #&gt; 153 20 68 Note: using c() function we can form any vector and that will enable display of the respective columns. We did not specify the row, so all rows will be displayed. Five Number summary summary(airquality$Temp) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 56.00 72.00 79.00 77.88 85.00 97.00 Histogram hist(airquality$Temp) We can use the plot() function to just plot plot(airquality$Temp) We can use the plot() function to get a Scatter plot plot(airquality$Ozone, airquality$Temp) Plotting the whole dataset plot(airquality) 1.5.4 Data frames in R df &lt;- data.frame(x = 1:10, y = 2:11) 1.5.5 The tidyverse-package 1.5.6 Reading Data frames into R 1.5.7 Generating Random Data sets in R # df &lt;- runif() 1.5.8 Working With dplyr-package library(&quot;dplyr&quot;) The Master.csv file contains Deceased data from Karnataka COVID-19 Bulletin decdf &lt;- read.csv( file = &quot;https://www.isibang.ac.in/~athreya/Teaching/ISCD/Master.csv&quot;, header = TRUE ) head(decdf) #&gt; Sno District State.P.No Age.In.Years Sex #&gt; 1 1 Kalaburagi 6 76 Male #&gt; 2 2 Chikkaballapura 53 70 Female #&gt; 3 3 Tumakuru 60 60 Male #&gt; 4 4 Bagalakote 125 75 Male #&gt; 5 5 Kalaburagi 177 65 Male #&gt; 6 6 Gadag 166 80 Female #&gt; Description Symptoms Co.Morbidities DOA DOD #&gt; 1 Travel history to Saudi Arabia &lt;NA&gt; HTN &amp; Asthama &lt;NA&gt; &lt;NA&gt; #&gt; 2 Travel history to Mecca &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 3 Travel history to Delhi &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2020-04-03 #&gt; 5 SARI &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; 6 SARI &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; #&gt; MB.Date Notes #&gt; 1 2020-03-13 &lt;NA&gt; #&gt; 2 2020-03-26 &lt;NA&gt; #&gt; 3 2020-03-27 &lt;NA&gt; #&gt; 4 2020-04-04 &lt;NA&gt; #&gt; 5 2020-04-08 &lt;NA&gt; #&gt; 6 2020-04-09 &lt;NA&gt; names(decdf) &lt;- c( &quot;Sno&quot;, &quot;District&quot;, &quot;Pid&quot;, &quot;Age&quot;, &quot;Sex&quot;, &quot;Description&quot;, &quot;Symptoms&quot;, &quot;CMB&quot;, &quot;DOA&quot;, &quot;DOD&quot;, &quot;MB.Date&quot;, &quot;Notes&quot; ) Some imporatnt dplyr functions: 1.5.8.1 filter(): Extract rows that meet logical criteria filters data according to the given condition Filters data by age greater than 100 filter(decdf, Age &gt; 100) #&gt; Sno District Pid Age Sex Description #&gt; 1 3277 Bengaluru Urban 180841 102 Male ILI #&gt; 2 17972 Bengaluru Rural 1361618 102 Male SARI #&gt; 3 24686 Bengaluru Urban 1341967 102 Male SARI #&gt; 4 27273 Bengaluru Urban 2360283 102 Female SARI #&gt; 5 33704 Mysuru 2807010 110 Male SARI #&gt; 6 34793 Haveri 2843699 101 Male SARI #&gt; 7 35077 Kolar 2816836 103 Male ILI #&gt; 8 37190 Kodagu 2947715 101 Female SARI #&gt; 9 37373 Uttara Kannada 2996149 102 Male ILI #&gt; Symptoms CMB DOA DOD MB.Date #&gt; 1 Fever, Cough CKD, IHD 2020-08-08 2020-08-08 2020-08-10 #&gt; 2 Breathlessness DM, HTN 2021-04-24 2021-04-25 2021-05-08 #&gt; 3 Breathlessness DM 2021-04-24 2021-05-04 2021-05-23 #&gt; 4 Breathlessness - 2021-05-11 2021-05-25 2021-05-27 #&gt; 5 Fever, Cough, Breathlessness - 2021-06-12 2021-06-17 2021-06-19 #&gt; 6 Fever, Cough, Breathlessness DM, HTN 2021-06-18 2021-06-28 2021-06-28 #&gt; 7 Fever, Cough - 2021-06-14 2021-06-30 2021-07-01 #&gt; 8 Fever, Cough, Breathlessness - 2021-08-01 2021-08-26 2021-08-27 #&gt; 9 Fever, cough HTN &lt;NA&gt; 2021-09-02 2021-09-06 #&gt; Notes #&gt; 1 &lt;NA&gt; #&gt; 2 &lt;NA&gt; #&gt; 3 &lt;NA&gt; #&gt; 4 &lt;NA&gt; #&gt; 5 &lt;NA&gt; #&gt; 6 &lt;NA&gt; #&gt; 7 &lt;NA&gt; #&gt; 8 &lt;NA&gt; #&gt; 9 Died at his residence Retains only the rows satisfying the given conditions filter(decdf, Age &gt; 100 &amp; Sex == &quot;Female&quot;) #&gt; Sno District Pid Age Sex Description #&gt; 1 27273 Bengaluru Urban 2360283 102 Female SARI #&gt; 2 37190 Kodagu 2947715 101 Female SARI #&gt; Symptoms CMB DOA DOD MB.Date Notes #&gt; 1 Breathlessness - 2021-05-11 2021-05-25 2021-05-27 &lt;NA&gt; #&gt; 2 Fever, Cough, Breathlessness - 2021-08-01 2021-08-26 2021-08-27 &lt;NA&gt; head(decdf$DOA) #&gt; [1] NA NA NA NA NA NA head(decdf$MB.Date) #&gt; [1] &quot;2020-03-13&quot; &quot;2020-03-26&quot; &quot;2020-03-27&quot; &quot;2020-04-04&quot; &quot;2020-04-08&quot; #&gt; [6] &quot;2020-04-09&quot; Drop the NA rows decdf &lt;- filter(decdf, !is.na(DOD)) Can’t be done with subset() 1.5.8.2 mutate(): To add new variable without affecting original ones decdf &lt;- mutate( decdf, reporting.time = as.Date(decdf$MB.Date) - as.Date(decdf$DOD) # Here we have added new variable &quot;reporting.time&quot; to the dataframe # Original variables are not affected ) Similarly added a new variable “months” decdf &lt;- mutate(decdf, Month = months(as.Date(decdf$MB.Date)) ) 1.5.8.3 distinct(): Removes rows with duplicate values Selects distinct rows of Age variable DT &lt;- distinct(decdf, Age) Other variables can be kept with .keep_all = TRUE argument DT &lt;- distinct(decdf, Age, .keep_all = TRUE) 1.5.8.4 slice(): Select rows by position SL &lt;- slice(decdf, 10:12) head(SL, 2) #&gt; Sno District Pid Age Sex Description Symptoms #&gt; 1 24 Bidar 590 82 Male SARI &lt;NA&gt; #&gt; 2 25 Bengaluru Urban 557 63 Male &lt;NA&gt; Breathlessness #&gt; CMB DOA DOD MB.Date #&gt; 1 &lt;NA&gt; 2020-04-27 2020-04-28 2020-05-02 #&gt; 2 Diabetes &amp; Hypertension &amp; Hypothyroidism 2020-04-30 2020-05-02 2020-05-02 #&gt; Notes reporting.time Month #&gt; 1 &lt;NA&gt; 4 days May #&gt; 2 &lt;NA&gt; 0 days May 1.5.8.5 group_by(): To create a “grouped” copy of a table grouped by columns in … dplyr functions will manipulate each “group” separately and combine the results. GS &lt;- group_by(decdf, Sex) groups data by the specified variable. head(GS) #&gt; # A tibble: 6 × 14 #&gt; # Groups: Sex [2] #&gt; Sno District Pid Age Sex Descr…¹ Sympt…² CMB DOA DOD MB.Date #&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 4 Bagalakote 125 75 Male &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2020… 2020-0… #&gt; 2 11 Chikkaballa… 250 65 Male &lt;NA&gt; H1N1 p… DM &amp;… 2020… 2020… 2020-0… #&gt; 3 13 Bengaluru U… 195 66 Male &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2020… 2020… 2020-0… #&gt; 4 14 Vijayapura 374 42 Male &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 2020… 2020… 2020-0… #&gt; 5 19 Bengaluru U… 465 45 Fema… SARI Pneumo… Daib… 2020… 2020… 2020-0… #&gt; 6 20 Kalaburagi 422 57 Male SARI &lt;NA&gt; CLD 2020… 2020… 2020-0… #&gt; # … with 3 more variables: Notes &lt;chr&gt;, reporting.time &lt;drtn&gt;, Month &lt;chr&gt;, and #&gt; # abbreviated variable names ¹​Description, ²​Symptoms #&gt; # ℹ Use `colnames()` to see all variable names NOTE: Display does NOT show grouping, but it will specify the groups 1.5.8.6 summarise(): Compute table of summaries Summarises multiple values into a single value Gives the mean of age for each gender. summarise(GS, mean(Age, na.rm = TRUE)) #&gt; # A tibble: 10 × 2 #&gt; Sex `mean(Age, na.rm = TRUE)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 F  65.2 #&gt; 2 Female 60.7 #&gt; 3 M  66.1 #&gt; 4 M E23 71 #&gt; 5 Male 60.7 #&gt; 6 N 39 #&gt; # … with 4 more rows #&gt; # ℹ Use `print(n = ...)` to see more rows 1.5.8.7 sample_n(): To select random rows according to the value specified Selects 2 random rows from dataframe decdf. sample_n(decdf, size = 2) #&gt; Sno District Pid Age Sex Description #&gt; 1 15681 Bengaluru Urban 1394442 53 Female ILI #&gt; 2 11875 Chikkaballapura 896738 65 Male SARI #&gt; Symptoms CMB DOA DOD MB.Date Notes #&gt; 1 Fever HTN 2021-04-24 2021-04-26 2021-05-01 &lt;NA&gt; #&gt; 2 Fever, Cough, Breathlessness DM, HTN 2020-11-28 2020-12-05 2020-12-08 &lt;NA&gt; #&gt; reporting.time Month #&gt; 1 5 days May #&gt; 2 3 days December Selects 0.0001-fraction of rows at random. sample_frac(decdf, size = 0.0001) #&gt; Sno District Pid Age Sex Description #&gt; 1 14824 Bengaluru Urban 1121723 82 Female SARI #&gt; 2 4938 Tumakuru 283766 45 Female SARI #&gt; 3 25424 Vijayapura 1744251 59 Male ILI #&gt; 4 9888 Bengaluru Urban 694955 36 Male SARI #&gt; Symptoms CMB DOA DOD MB.Date Notes #&gt; 1 Breathlessness DM, HTN 2021-04-14 2021-04-25 2021-04-28 &lt;NA&gt; #&gt; 2 Cough, Breathlessness DM, COPD 2020-08-22 2020-08-22 2020-08-25 &lt;NA&gt; #&gt; 3 Fever, Cough DM, HTN 2021-04-30 2021-05-20 2021-05-24 &lt;NA&gt; #&gt; 4 Fever, Cough, Breathlessness HIV 2020-09-29 2020-10-08 2020-10-10 &lt;NA&gt; #&gt; reporting.time Month #&gt; 1 3 days April #&gt; 2 3 days August #&gt; 3 4 days May #&gt; 4 2 days October 1.5.8.8 count(): To count the unique values of one or more variables Gives a frequency table for months count(decdf, Month) #&gt; Month n #&gt; 1 April 2963 #&gt; 2 August 4107 #&gt; 3 December 429 #&gt; 4 February 481 #&gt; 5 January 837 #&gt; 6 July 3561 #&gt; 7 June 6046 #&gt; 8 March 236 #&gt; 9 May 13592 #&gt; 10 November 736 #&gt; 11 October 2593 #&gt; 12 September 3643 #&gt; 13 &lt;NA&gt; 17 1.5.8.9 arrange(): Order rows by values of a column or columns (low to high) use with desc() to order from high to low orderdf &lt;- arrange(decdf, Age) Creates a new dataframe orderdf having rows arranged by - Age. head(orderdf, 2) #&gt; Sno District Pid Age Sex Description Symptoms CMB #&gt; 1 14253 Bengaluru Urban 1260623 0.0000 Male SARI Breathlessness HTN #&gt; 2 20970 Ramanagara 2032210 0.0082 Female SARI Breathlessness - #&gt; DOA DOD MB.Date Notes reporting.time Month #&gt; 1 2021-04-21 2021-04-23 2021-04-24 &lt;NA&gt; 1 days April #&gt; 2 2021-05-07 2021-05-10 2021-05-14 &lt;NA&gt; 4 days May Arranges the data in alphabetical order of the variable - Description orderdf2 &lt;- arrange(decdf, Description) 1.5.8.10 The pipe operator - %&gt;% Used to chain codes x %&gt;% f(y) becomes f(x, y) filtered_data &lt;- filter(decdf, Month != &quot;September&quot;) grouped_data &lt;- group_by(filtered_data, Month) summarise(grouped_data, mean(Age, na.rm = TRUE)) #&gt; # A tibble: 11 × 2 #&gt; Month `mean(Age, na.rm = TRUE)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 April 61.2 #&gt; 2 August 61.3 #&gt; 3 December 64.9 #&gt; 4 February 65.3 #&gt; 5 January 63.6 #&gt; 6 July 60.0 #&gt; # … with 5 more rows #&gt; # ℹ Use `print(n = ...)` to see more rows The same code written shortly with Pipe - %&gt;% decdf %&gt;% filter(Month != 5) %&gt;% group_by(Month) %&gt;% summarise(mean(Age, na.rm = TRUE)) #&gt; # A tibble: 12 × 2 #&gt; Month `mean(Age, na.rm = TRUE)` #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 April 61.2 #&gt; 2 August 61.3 #&gt; 3 December 64.9 #&gt; 4 February 65.3 #&gt; 5 January 63.6 #&gt; 6 July 60.0 #&gt; # … with 6 more rows #&gt; # ℹ Use `print(n = ...)` to see more rows Esercises Use the dplyr package to do the following computations. Create a new data frame iris1 that contains only the species virginica and versicolor with sepal lengths longer than 6cm and sepal widths longer than 2.5cm. How many observations and variables are in the dataset? Now, create a iris2 data frame from iris1 that contains only the columns for Species, Sepal.Length, and Sepal.Width. How many observations and variables are in the dataset? Create an iris3 data frame from iris2 that orders the observations from largest to smallest sepal length. Show the first 6 rows of this dataset. Create an iris4 data frame from iris3 that creates a column with a Sepal.Area (length * width) value for each observation. How many observations and variables are in the dataset? Create iris5 that calculates the average sepal length, the average sepal width, and the sample size of the entire iris4 data frame and print iris5. Finally, create iris6 that calculates the average sepal length, the average sepal width, and the sample size for each species of in the iris4 data frame and print iris6. In these exercises, you have successively modified different versions of the data frame iris1, iris2, iris3, iris4, iris5, iris6. At each stage, the output data frame from one operation serves as the input fro the next. A more easy way to do this is to use the pipe operator %&gt;% from the tidyr package. Rework all of your previous statements into an extended piping operation that uses irisas the input and generates iris6 as the output. "],["2-review-of-basic-probability.html", "Chapter 2 Review of Basic probability ", " Chapter 2 Review of Basic probability "],["2.1-recalling-the-distributions-expectation-variance.html", "2.1 Recalling the distributions, Expectation, Variance", " 2.1 Recalling the distributions, Expectation, Variance "],["2.2-demoivre-central-limit-theorem.html", "2.2 DeMoivre Central Limit Theorem", " 2.2 DeMoivre Central Limit Theorem "],["2.3-tchebychev-inequality.html", "2.3 Tchebychev Inequality", " 2.3 Tchebychev Inequality "],["2.4-empirical-distribution-function.html", "2.4 Empirical Distribution Function", " 2.4 Empirical Distribution Function "],["2.5-sample-mean-and-variance.html", "2.5 Sample Mean and Variance", " 2.5 Sample Mean and Variance "],["2.6-chi2-t-and-f-distribution.html", "2.6 \\(\\chi^2, t\\) and \\(F\\) distribution", " 2.6 \\(\\chi^2, t\\) and \\(F\\) distribution "],["3-statistical-estimation.html", "Chapter 3 Statistical Estimation", " Chapter 3 Statistical Estimation [Sherlock Holmes:] “The temptation to form premature theories upon insufficient data is the bane of our profession.” — Arthur Conan Doyle (The Valley of Fear) "],["3.1-estimator.html", "3.1 Estimator", " 3.1 Estimator "],["3.2-method-of-moments.html", "3.2 Method of moments", " 3.2 Method of moments "],["3.3-maximum-liklihood-estimate.html", "3.3 Maximum liklihood estimate", " 3.3 Maximum liklihood estimate "],["3.4-confidence-intervals.html", "3.4 Confidence Intervals", " 3.4 Confidence Intervals "],["3.5-bivariate-data.html", "3.5 Bivariate Data", " 3.5 Bivariate Data "],["3.6-covariance-correlation.html", "3.6 Covariance, Correlation", " 3.6 Covariance, Correlation "],["3.7-method-of-least-squares-linear-regression.html", "3.7 Method of least squares (Linear regression)", " 3.7 Method of least squares (Linear regression) "],["3.8-t-confidence-interval.html", "3.8 \\(t\\)-confidence Interval", " 3.8 \\(t\\)-confidence Interval "],["4-hypothesis-testing.html", "Chapter 4 Hypothesis Testing", " Chapter 4 Hypothesis Testing “It’s easy to lie with statistics. It’s hard to tell the truth without statistics” — Andrejs Dunkels "],["4.1-introduction.html", "4.1 Introduction", " 4.1 Introduction So far we have discussed models especially Linear Model where ingridients were population of interest, smaple data from the population and more importantly we assumed that the data came from certain model, so we estimated the parameters corresponding to the model. In this chapter, we will shift our focus and will take an interest to the following questions: Are two sub-population “different” or “same”? Are the measured attributes independent of each other? For example, Are temperatures today higher than they were 100 years ago? Does smoking reduce life expectency? Is treatment A genuinely different from treatment B? In all the above three cases, you take an interest in making an inference about how the value of the parameter relates to a specific numerical value. Is it less than, equal to or greater than a specified given number? This kind of inference is called test of hypothesis. 4.1.1 Elements of Hypothesis testing Definition 4.1 (Hypothesis) A statistical hypothesis is a statement about the numerical value of a population parameter. What hypothesis testing is? You hypothesise some statement about the data and develop a test which will tell you wheather the hypothesis is resonable or not. And, these are the key steps of hypothesis testing. To execute a hypothesis testing we will first, Make a conjecture second, Perform some probabilistic and statistical computation to test the conjecture. Here you will see many application of those probabilistic distributions you learnt earlier, appearing in different tests. . In the computation step you will compute the likeliness of the Let’s see an example: Given a coin, we are interested in the probability \\(p\\) of showing heads when tossed. We toss the coin 100 times and record the outcome as \\(X_1, X_2, \\ldots, X_{100} \\overset{\\mathrm{iid}}{\\sim} Ber(p)\\) and found that \\(\\sum_{i=1}^{100} X_i = 67\\). Till now we used this (in MOM and MLE) to estimate \\(p\\). Now we ask a diiferent question: Is \\(p=0.5\\) a valid hypothesis given the finding? ,that is, we conjectured that \\(p=0.5\\) and want to Observe that, If the coin had an equal chance of heads and tails then chance of 67 heads in 100 tosses is \\[\\binom{100}{67} \\left( \\frac{1}{2} \\right)^{100} \\approx 0.04\\] Anyway let’s directly dig into hypothesis test, z-test: Suppose \\(X_1, X_2, \\ldots, X_n \\overset{\\mathrm{iid}}{\\sim} N(\\mu, \\sigma^2)\\) where, \\(\\sigma\\) is known but \\(\\mu\\) is unknown. Let \\(X_1, X_2, \\ldots, X_n\\) be iid sample from the population. We compute and find: \\[\\overline{X} = \\frac{X_1 + X_2 + \\cdots + X_n}{n}\\] Is \\(\\underset{\\mathrm{null \\ hypothesis}}{\\mu = c}\\) or \\(\\underset{\\mathrm{alternate \\ hypothesis}}{\\mu &gt; c}\\)? Now the question comes, Given \\(\\overline{X}\\); If null hypothesis was true how likely is is that we would have a sample mean as large as the observed value \\(\\overline{X}\\)? Answer: \\(X_1, X_2, \\ldots, X_n\\) are known. Let \\(Y_1, Y_2, \\ldots, Y_n\\overset{\\mathrm{iid}}{\\sim} N(c, \\sigma^2)\\) \\[\\text{Compute:} \\quad \\mathbb{P}\\left(\\underset{\\mathrm{Random \\ variable}}{\\overline{Y}} \\geq \\underset{\\mathrm{Determistic \\ quantity}}{\\overline{X}}\\right)\\] \\(\\mathbb{P}({\\overline{Y}} \\geq \\overline{X})\\) is called the test statistic which descrobes how likely it is that test statistic \\(\\overline{X}\\) would be atleast as far away form \\(c\\) as what was observed. Define: \\[Z := \\frac{\\sqrt{n}(\\overline{Y}-c)}{\\sigma} \\sim N(0, 1)\\] Now, \\[\\begin{align*} &amp;\\mathbb{P}({\\overline{Y}} \\geq \\overline{X}) \\\\ &amp; = \\mathbb{P}\\left(\\frac{\\sqrt{n}(\\overline{Y}-c)}{\\sigma} \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) \\\\ &amp; = \\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) \\end{align*}\\] We reject the null hypothesis if the probability is “small” (variable at the hands of the user!) So we fix level \\(\\alpha \\in (0,1)\\). If \\(\\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) &lt; \\alpha\\) then, we conclude that the sample average \\(\\overline{X}\\) is so far from \\(c\\) that hypothesis \\(\\mu = c\\) is not true! For example, Suppose \\(X \\sim N(\\mu, 9)\\). A sample \\(X_1, X_2, \\ldots, X_{16}\\) is drawn from \\(X\\) and we observe \\(\\overline{X} = 10.2\\) Null hypothesis: \\(\\mu = 9.5\\) Alternate hypothesis: \\(\\mu &gt; 9.5\\) Level of significance: \\(\\alpha = 0.5\\) Answer: \\(\\overline{X} = 10.2; c = 9.5; \\sigma = 3; \\mu = 16\\) Compute: \\[\\begin{align*} &amp; \\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) \\\\ &amp; = \\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{16}(10.2-9.5)}{3}\\right) \\\\ &amp; = \\mathbb{P}\\left(Z \\geq \\frac{4\\times 0.7}{3}\\right) \\\\ &amp; \\approx 0.175 \\tag{from $Z-$table} \\end{align*}\\] Observe: For, \\(\\alpha = 0.05\\) we have \\(\\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) \\geq \\alpha\\). Conclusion: At level \\(\\alpha\\) we can’t reject the null hypothesis. Example 2: Suppose \\(X \\sim N(\\mu, 9)\\). A sample \\(X_1, X_2, \\ldots, X_{16}\\) is drawn from \\(X\\) and we observe \\(\\overline{X} = 10.2\\) Null hypothesis: \\(\\mu = 8.5\\) Alternate hypothesis: \\(\\mu &gt; 8.5\\) Level of significance: \\(\\alpha = 0.5\\) Answer: \\(\\overline{X} = 10.2; c = 8.5; n = 16; \\sigma = 3; \\mu = 16\\) Compute: \\[\\begin{align*} &amp; \\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) \\\\ &amp; = \\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{16}(10.2-8.5)}{3}\\right) \\\\ &amp; = \\mathbb{P}\\left(Z \\geq \\frac{4\\times 1.7}{3}\\right) \\\\ &amp; \\approx 0.012 \\tag{from $Z-$table} \\end{align*}\\] Observe: For, \\(\\alpha = 0.05\\) we have \\(\\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) = 0.012 &lt; \\alpha\\). Conclusion: At level \\(\\alpha\\) as \\(\\mathbb{P}\\left(Z \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right)\\) is less than \\(\\alpha\\) , we reject the null hypothesis. Example 3: Suppose a medical research team wants to design an experiment to determine wheather the newly developed vaccine for a new disease is effective or not? Experiment (First cut strategy): Choose: \\(n-\\)individuals from the population. \\(n_1\\) of them are given the vaccine, \\(n_2(= n-n_1)\\) of them are given the placebo. Wait a specific amount of time to see how many are affected by the disease. Summarize the findings as a \\(2\\times 2\\) table: Infected Not infected Vaccine \\(X_{11}\\) \\(X_{12}\\) \\(n_1\\) Placebo \\(X_{21}\\) \\(X_{22}\\) \\(n_2\\) If the vaccine was effective: Expect a smaller proportion of the vaccinated group to be affected by the disease. If the chance of getting affected by the disease doesn’t depend on wheather the vaccine was given or no, then the vaccine is ineffective. Approach: Start by assuming independence ,i.e., the vaccine has no effect unless convinced otherwise by evidence. General question/Situation: We have two “treatments” applied to a group of experimental units. One of two possible outcomes is record. \\[ X_{ij} = \\begin{cases} \\text{\\# of participants given treatment $i$ and had outcome $j$} \\end{cases} \\] Note: In example above there is an exlicit randomness. Assume: \\(n-\\)fixed. Choose \\(n_1-\\)randomly without replacement from \\(n\\). 4.1.2 Exercise Suppose we assume chance of getting the disease is \\(p\\). What is the distribution of \\(X_{11}\\)? Decision Making(Ideal): If \\(X_{11}\\) is the among more liekly possiblities (under the independence/vaccine ineffective) then we have no reason to suspect the ineffectiveness of the vaccine. On the other hand, if \\(X_{11}\\) is the among “impossible” possiblities (under the independence/vaccine ineffective) then we have reason to reject the hypothesis of the ineffectiveness of the vaccine. Now another approach is to test in the parametric setup (more intuitive). Example: Given a coin and we are interested in the probability \\(p\\) of showing heads when tossed. Toss teh coin \\(100\\) times. Record sample data as \\(X_1, X_2, \\ldots, X_{100}\\) with \\(X_i \\sim Ber(p)\\). Get that \\(\\sum_{i=1}^{100} = 67\\) Now the same question comes, \\(p=0.5\\) or \\(p \\not= 0.5\\)? Answer: Compute \\[ \\mathbb{P}\\left(\\sum_{i=1}^{100} = 67 \\right) \\ \\text{under $p = 0.5$} \\] Depending on the answer conclude the hypothesis accurate or not Find MLE of \\(p\\) given \\(X_1, X_2, \\ldots, X_{100}\\) Provide a confidence interval for \\(p\\) Broad procedure to a test: \\(X_1, X_2, \\ldots, X_{100}\\) are \\(\\rm iid\\) sample from \\(X\\) \\(X\\) has pmf/pdf \\(f(x|p)\\) \\(p \\in \\mathcal{P} \\subseteq \\mathbb{R}^{\\theta}\\) Hypothesis that we posed: p=0.5; Restrict the values \\(p\\) can take, say \\(p \\in \\mathcal{P} \\not\\subseteq \\mathcal{P}\\) Device a computation to test the hypothesis, i.e., find a test statistic \\(\\equiv\\) function of sample \\(X_1, X_2, \\ldots, X_{n}\\) "],["4.2-z-and-t-test.html", "4.2 \\(z\\) and \\(t\\) test", " 4.2 \\(z\\) and \\(t\\) test 4.2.1 \\(z-\\)test: Population is \\(N(\\mu, \\sigma^2)\\) and \\(\\mu\\) unknown Suppose - \\(X_1, X_2, \\ldots, X_{n} \\overset{\\mathrm{iid}}{\\sim} N(\\mu, \\sigma^2)\\) where \\(\\sigma\\) is known but \\(\\mu\\) is unknown. Let \\(\\mu \\in \\mathcal{P} = \\mathbb{R}\\) Hypothesis: \\(\\mu = c \\ \\text{,i.e.,} \\ \\ \\mathcal{P} = \\{c\\} \\subset \\mathcal{P}\\) Intuitive test statistic: \\(\\overline{X}\\) is the estimator of \\(\\mu = c\\) Naive test Check if \\(\\overline{X} = c\\). Say “\\(\\overline{X}\\) is close to \\(c\\)”(depends on \\(\\sigma\\)). Is there a better approach with \\(\\overline{X}?\\) Exercise 4.1 \\(\\overline{X} \\sim N(c, \\frac{\\sigma^2}{n})\\) if \\(\\mu = c\\) ,i.e., hypothesis is true. which would result, \\(\\sqrt{n}\\left(\\frac{\\overline{X}-c}{\\sigma}\\right) \\sim N(0,1)\\) Another way to express “if” statement: If \\(Y_1, Y_2, \\ldots, Y_{n} \\overset{\\mathrm{iid}}{\\sim} N(c, \\sigma^2)\\) then \\(\\sqrt{n}\\left(\\frac{\\overline{Y}-c}{\\sigma}\\right) \\sim N(0,1)\\) Here \\(Y_i\\)’s are hypothetical random variables and \\(N(c, \\sigma^2)\\) is called “Null distribution” Computing, \\[\\begin{equation} \\mathbb{P}\\left(\\frac{\\sqrt{n}(\\overline{Y}-c)}{\\sigma} \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right) \\tag{*} \\end{equation}\\] allows us to compute, Hypothesis 4.1 \\(H_0: \\mu = c\\) vs \\(H_A: \\mu &gt; c\\) The \\(p-\\)value is the value of \\((*)\\) Fix \\(\\alpha \\in (0,1)\\) (level of significance). If \\(p-\\)value \\(&lt; \\alpha\\) then reject the null hypothesis \\(\\mu = c\\) in favour of alternate \\(\\mu &gt; c\\). If \\(p-\\)value \\(\\geq \\alpha\\) then there is no evidence to reject the null hypothesis \\(\\mu = c\\) in favour of alternate \\(\\mu &gt; c\\). Exercise 4.2 Device a computation/test of the following: \\(H_0: \\mu = c\\) vs \\(H_A: \\mu &lt; c\\) \\(H_0: \\mu = c\\) vs \\(H_A: \\mu \\not= c\\) Hint: Suitably alter the computation \\(\\mathbb{P}\\left(\\frac{\\sqrt{n}(\\overline{Y}-c)}{\\sigma} \\geq \\frac{\\sqrt{n}(\\overline{X}-c)}{\\sigma}\\right)\\) 4.2.2 \\(t-\\)test: Test for sample mean when variance is unknown Assume \\(X \\sim N(\\mu, \\sigma^2)\\) &amp; both \\(\\mu\\) and \\(\\sigma\\) are unknown. Let \\(X_1, X_2, \\ldots, X_{n} \\overset{\\mathrm{iid}}{\\sim} N(\\mu, \\sigma^2)\\) Hypothesis 4.2 \\(H_0: \\mu=c\\) vs \\(H_A: \\mu &lt; c\\) Let \\(Y_1, Y_2, \\ldots, Y_{n}\\) be random variables that “mimic” the sampling procedure. \\(Y \\sim N(c, s^2)\\) Under, \\(H_0\\) ,i.e., assume \\(\\mu = c\\) \\[ \\sqrt{n}\\left(\\frac{\\overline{Y}-c}{S} \\right) \\sim t_{n-1} \\] \\[\\begin{align*} &amp;\\mathbb{P}({\\overline{Y}} &lt; \\overline{X}) \\\\ &amp; = \\mathbb{P}\\left(\\frac{\\sqrt{n}(\\overline{Y}-c)}{S} &lt; \\frac{\\sqrt{n}(\\overline{X}-c)}{S}\\right) \\\\ &amp; = \\mathbb{P}\\left(T &lt; \\frac{\\sqrt{n}(\\overline{X}-c)}{S}\\right) = ? \\end{align*}\\] where \\(T \\sim t_{n-1}\\) Fix \\(\\alpha \\in (0,1)\\). If \\(\\mathbb{P}\\left(T &lt; \\frac{\\sqrt{n}(\\overline{X}-c)}{S}\\right) &lt; \\alpha\\) then reject \\(H_0\\) Exercise 4.3 Prescribe the \\(t-\\)test when \\(H_0: \\mu = c\\) vs \\(H_A: \\mu &lt; c\\) \\(H_0: \\mu = c\\) vs \\(H_A: \\mu \\not= c\\) 4.2.3 Log likelihood In the genral approach, We assume that som random variable \\(X\\) has a pdf/pmf \\(f(\\cdot | p)\\) with \\(p \\in \\mathcal{P} \\subseteq \\mathbb{R}\\). Sample \\(X_1, X_2, \\ldots, X_{n} \\overset{\\mathrm{iid}}{\\sim} X\\) the likelihood function of the given sample \\(X_1, X_2, \\ldots, X_{n}\\) is \\[L(p; X_1, X_2, \\ldots, X_{n}) = \\prod_{i=1}^{n} f(X_i | p)\\] Recall that MLE, \\(\\hat{p} = {\\rm argmax}_{p \\in \\mathcal{P}}L(p; X_1, X_2, \\ldots, X_{n})\\) Observe that we can view the hypothesis test as a restriction of \\(\\mathcal{P}\\) to a smaller subset \\(\\mathcal{P}_0\\) For example, \\(\\mathcal{P} = \\{c\\}\\) in the “intuitive approach” above, \\[ H_0: \\underset{(\\mu = c)}{p \\in \\mathcal{P}_0} \\qquad \\qquad H_A: \\underset{(\\mu \\not= c)}{p \\not\\in \\mathcal{P}_0} \\] 4.2.4 MLE approach under null hypothesis with \\(p \\in \\mathcal{P}_0 \\subset \\mathcal{P}\\) \\[\\hat{p} = \\underset{p \\in \\mathcal{P}}{\\rm argmax} L(p; X_1, X_2, \\ldots, X_{n})\\] Likelihood ratio: Given, a sample \\(X_1, X_2, \\ldots, X_{n}\\), define \\[ \\lambda(X_1, X_2, \\ldots, X_{n}) = \\frac{L(\\hat{p}_0; X_1, X_2, \\ldots, X_{n})}{L(\\hat{p}; X_1, X_2, \\ldots, X_{n})} \\] as the Likelihood ratio and define, \\[\\begin{align*} \\Lambda(X_1, X_2, \\ldots, X_{n}) &amp;= -\\log{\\lambda(X_1, X_2, \\ldots, X_{n})} \\\\ &amp;= -\\log{\\frac{L(\\hat{p}_0; X_1, X_2, \\ldots, X_{n})}{L(\\hat{p}; X_1, X_2, \\ldots, X_{n})}} \\\\ \\end{align*}\\] as log-likelihood ratio. Exercise 4.4 \\[ \\text{If } \\mathcal{P}_0 \\subseteq \\mathcal{P} \\text{ then } 0 \\leq \\frac{L(\\hat{p}_0; X_1, X_2, \\ldots, X_{n})}{L(\\hat{p}; X_1, X_2, \\ldots, X_{n})} \\leq 1 \\] which would imply, \\[\\begin{align*} 0 &amp;\\leq \\Lambda(X_1, X_2, \\ldots, X_{n})\\\\ &amp;= -\\log{\\frac{L(\\hat{p}_0; X_1, X_2, \\ldots, X_{n})}{L(\\hat{p}; X_1, X_2, \\ldots, X_{n})}}\\\\ &amp;= \\log{\\frac{L(\\hat{p}; X_1, X_2, \\ldots, X_{n})}{L(\\hat{p}_0; X_1, X_2, \\ldots, X_{n})}} \\end{align*}\\] If \\(\\hat{p}\\) is further away from \\(\\mathcal{P}_0\\) in terms of \\(L\\) then less likely it is that \\(\\mathcal{P}_0\\) is true as the null hypothesis ,i.e., for larger values of \\(\\Lambda\\) 4.2.5 \\(z-\\)test with log-liklihood Let, \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\mu \\in \\mathcal{P} = \\mathbb{R}\\) and \\(\\sigma\\) is known. The null and alternate hypothesis be \\(H_0: \\mu = c\\) and \\(H_A: \\mu \\not= c\\) ,i.e., \\(\\mathcal{P}_0 = \\{c\\}\\). Now given, a sample \\(X_1, X_2, \\ldots, X_{n}\\), the log-likelihood function for \\(\\mu\\), \\[ L(\\mu; X_1, X_2, \\ldots, X_{n}) = \\prod_{i=1}^{n} \\frac{e^{-\\frac{(X_i - \\mu)^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma} \\] Exercise 4.5 Show that, \\[\\hat{\\mu} = \\underset{\\mu \\in \\mathcal{P}}{\\rm argmax} \\ L(\\mu; X_1, X_2, \\ldots, X_{n}) = \\overline{X}\\] and, \\[\\hat{\\mu}_0 = \\underset{\\mu \\in \\mathcal{P}_0}{\\rm argmax} \\ L(\\mu; X_1, X_2, \\ldots, X_{n}) = c\\] then we will have, \\[\\begin{align*} \\Lambda(X_1, X_2, \\ldots, X_{n}) &amp;= \\log{\\frac{L(\\mu; X_1, X_2, \\ldots, X_{n})}{L(\\mu_0; X_1, X_2, \\ldots, X_{n})}} \\\\ &amp;= \\log{\\frac{L(\\overline{X}; X_1, X_2, \\ldots, X_{n})}{L(c; X_1, X_2, \\ldots, X_{n})}} \\\\ &amp;= \\log{\\frac{\\prod_{i=1}^{n} \\frac{e^{-\\frac{(X_i - \\overline{X})^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}}{\\prod_{i=1}^{n} \\frac{e^{-\\frac{(X_i - c)^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}}} \\\\ &amp;= \\frac{1}{2} \\frac{n}{\\sigma^2}(\\overline{X} - c)^2 = \\frac{1}{2} \\left( \\frac{\\sqrt{n}(\\overline{X} - c)}{\\sigma} \\right)^2 \\end{align*}\\] Exercise 4.6 Prove that, \\[ \\log{\\frac{\\prod_{i=1}^{n} \\frac{e^{-\\frac{(X_i - \\overline{X})^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}}{\\prod_{i=1}^{n} \\frac{e^{-\\frac{(X_i - c)^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma}}} = \\frac{1}{2}\\frac{n}{\\sigma^2}(\\overline{X} - c)^2 \\] Let \\(Y_1, Y_2, \\ldots, Y_{n}\\) be \\(\\rm iid\\) random variables that “imitate” sample under \\(H_0\\). We have to check, \\[ \\underbrace{\\mathbb{P} \\left( \\Lambda(Y_1, Y_2, \\ldots, Y_{n}) \\geq \\Lambda(X_1, X_2, \\ldots, X_{n}) \\right)}_{p-\\text{value of the test}} \\] We know, \\[\\begin{align*} \\Lambda(Y_1, Y_2, \\ldots, Y_{n}) \\\\ &amp;= \\frac{1}{2} \\left( \\frac{\\sqrt{n}(\\overline{Y} - c)}{\\sigma} \\right)^2 \\\\ &amp;= \\frac{Z^2}{2} \\end{align*}\\] where, \\(Z = \\frac{\\sqrt{n}(\\overline{Y} - c)}{\\sigma} \\sim N(0,1)\\) So, one can compute the \\(p-\\)value as \\(\\mathbb{P} \\left( Z^2 \\geq \\left( \\frac{\\sqrt{n}(\\overline{Y} - c)}{\\sigma} \\right)^2 \\right)\\) 4.2.6 Seeing the mean is larger that c or not Let \\(X \\sim N(\\mu, \\sigma^2)\\) with \\(\\sigma\\) known. \\(H_0: \\mu \\leq c\\) vs \\(H_A: \\mu &gt; c\\) Sample \\(X_1, X_2, \\ldots, X_{n}\\) from population and compute, \\[ \\Lambda(X_1, X_2, \\ldots, X_{n}) = \\log{\\frac{L(\\hat{\\mu}; X_1, X_2, \\ldots, X_{n})}{L(\\hat{\\mu}_0; X_1, X_2, \\ldots, X_{n})}} \\] where, \\(\\hat{\\mu}_0 = \\underset{\\mu \\in \\mathcal{P}_0}{\\rm argmax} \\ L(\\mu; X_1, \\ldots, X_n)\\) with \\(\\mathcal{P}_0 = (-\\infty, c]\\) and, \\(\\hat{\\mu} = \\underset{\\mu \\in \\mathcal{P}}{\\rm argmax} \\ L(\\mu; X_1, \\ldots, X_n)\\) with \\(\\mathcal{P} = \\mathbb{R}\\) Exercise 4.7 Show the following: \\(\\hat{\\mu} = \\overline{X}\\) \\(\\hat{\\mu}_0 = \\underset{\\mu \\in \\mathcal{P}_0}{\\rm argmax} \\ \\prod_{i=1}^{n} \\frac{e^{-\\frac{(X_i - \\mu)^2}{2\\sigma^2}}}{\\sqrt{2\\pi}\\sigma} = \\min{\\{\\overline{X}, c\\}}\\) \\[\\begin{align*} \\Lambda(X_1, X_2, \\ldots, X_{n}) &amp;= \\log{\\frac{L(\\hat{\\mu}; X_1, X_2, \\ldots, X_{n})}{L(\\hat{\\mu}_0; X_1, X_2, \\ldots, X_{n})}} \\\\ &amp;= \\begin{cases} 0 &amp;{\\rm if } \\ \\overline{X} \\leq c \\\\ \\frac{n(\\overline{X} - c)^2}{2\\sigma^2} &amp;{\\rm if } \\ \\overline{X} = c \\end{cases} \\end{align*}\\] And finally, we shall compute the \\(p-\\)value as, \\(\\mathbb{P} \\left( \\frac{\\sqrt{n}(\\overline{Y} - c)}{\\sigma} \\geq \\frac{\\sqrt{n}(\\overline{X} - c)}{\\sigma} \\right) = \\mathbb{P} \\left( Z \\geq \\frac{\\sqrt{n}(\\overline{X} - c)}{\\sigma} \\right)\\) "],["4.3-chi2-goodness-of-fit.html", "4.3 \\(\\chi^2\\) goodness of fit", " 4.3 \\(\\chi^2\\) goodness of fit "],["4.4-non-parametric-tests.html", "4.4 Non-parametric tests", " 4.4 Non-parametric tests "],["5-resampling-methods-jackknife-and-bootstrap.html", "Chapter 5 Resampling Methods: Jackknife and Bootstrap", " Chapter 5 Resampling Methods: Jackknife and Bootstrap Statistics is not a discipline like physics, chemistry or biology where we study a subject to solve problems in the same subject. We study statistics with the main aim of solving problems in other disciplines. — C. R. Rao "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
